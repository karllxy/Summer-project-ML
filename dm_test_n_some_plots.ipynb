{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the function dm_test from dm_test.py\n",
    "from dm_test import dm_test\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* since the mean r2oos of other methods including elastic net, glm, pls, neural network are too small, we only did a dm test on gbrt and rf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt_result_df = pd.read_pickle('result_data/gbrt_testing26_interaction_result.pkl')\n",
    "rf_result_df = pd.read_pickle('result_data/rf_testing26_interaction_result.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'permno', 'y_pred', 'y_real', 'params', 'pred_R2_OOS', 'CV_R2',\n",
       "       'CV_MSE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_result_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1985, 1986, 1987, 1988], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_result_df['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210315\n",
      "210315\n",
      "200050\n",
      "200050\n",
      "189885\n",
      "189885\n",
      "179910\n",
      "179910\n"
     ]
    }
   ],
   "source": [
    "for y in gbrt_result_df['year'].unique():\n",
    "    tmp_y_pred_array = gbrt_result_df.loc[gbrt_result_df['year'] == y, 'y_pred'].tolist()[0]\n",
    "    print(len(tmp_y_pred_array))\n",
    "    tmp_y_real_array = gbrt_result_df.loc[gbrt_result_df['year'] == y, 'y_real'].tolist()[0]\n",
    "    print(len(tmp_y_real_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recreate the train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.feather as feather\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the feather files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My current directory is : /home/spectre/Desktop/summer_project/Summer-project-ML\n",
      "My directory name is : Summer-project-ML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = os.getcwd()\n",
    "print(\"My current directory is : \" + directory_path)\n",
    "folder_name = os.path.basename(directory_path)\n",
    "print(\"My directory name is : \" + folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = os.getcwd()\n",
    "data_path = os.path.join(directory_path, 'data')  # the path to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_feather(os.path.join(data_path, 'chars60_raw_imputed.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adm',\n",
       " 'bm_ia',\n",
       " 'mom12m',\n",
       " 'mom36m',\n",
       " 'mom60m',\n",
       " 'mom6m',\n",
       " 'seas1a',\n",
       " 'abr',\n",
       " 'sue',\n",
       " 'cinvest',\n",
       " 'nincr',\n",
       " 'pscore',\n",
       " 'acc',\n",
       " 'bm',\n",
       " 'agr',\n",
       " 'alm',\n",
       " 'ato',\n",
       " 'cashdebt',\n",
       " 'chcsho',\n",
       " 'chpm',\n",
       " 'chtx',\n",
       " 'gma',\n",
       " 'grltnoa',\n",
       " 'lgr',\n",
       " 'ni',\n",
       " 'noa',\n",
       " 'op',\n",
       " 'pctacc',\n",
       " 'rna',\n",
       " 'roa',\n",
       " 'roe',\n",
       " 'rsup',\n",
       " 'sgr']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tf = raw_df.isnull().any()\n",
    "raw_tf.where(raw_tf == True).dropna().index.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['year'] = raw_df['date'].dt.year\n",
    "raw_df = raw_df[raw_df['year'] >= 1972]\n",
    "raw_df = raw_df.drop(['year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tf = raw_df.isnull().any()\n",
    "raw_tf.where(raw_tf == True).dropna().index.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read the macro data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_df = pd.read_csv(os.path.join(data_path, 'Macro.csv'))\n",
    "macro_df = macro_df[macro_df['yyyymm']>=197201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df['yyyymm'] = raw_df['date'].dt.strftime(\"%Y%m\").apply(int)\n",
    "total_df = pd.merge(raw_df, macro_df, how='inner', on='yyyymm')\n",
    "total_df.drop(columns=\"yyyymm\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tf = total_df.isnull().any()\n",
    "raw_tf.where(raw_tf == True).dropna().index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(raw_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of unique permno\n",
    "permno_list = total_df['permno'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to turn some variables to dummy variables\n",
    "def get_dummy_col(target_df, col_names):\n",
    "    \"\"\"turn the cols into multiple cols containing dummy variables, then drop the original columns\n",
    "\n",
    "    Args:\n",
    "        target_df (pandas dataframe): target dataframe\n",
    "        col_names (list): list of columns that need to be turned into dummy variables\n",
    "    \"\"\"\n",
    "    for col in col_names:        \n",
    "        tmp_dummies = pd.get_dummies(target_df[col]).rename(columns=lambda x: f'{col}_' + str(x))\n",
    "        target_df = pd.concat([target_df, tmp_dummies], axis=1)\n",
    "        target_df.drop(col, inplace=True, axis = 1)\n",
    "    return target_df \n",
    "dummy_col_list = ['ffi49']\n",
    "total_df = get_dummy_col(total_df, dummy_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set date as the index\n",
    "total_df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = total_df.columns.to_list()\n",
    "list_to_remove = ['gvkey', 'permno', 'ret', 'sic', 'exchcd', 'shrcd']\n",
    "for element in list_to_remove:\n",
    "    if element in features_list:\n",
    "        features_list.remove(element)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features_list = features_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = total_df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create interaction features(other feature * macro feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_for_interaction = total_df[features_list[:69]]\n",
    "single_features = features_list[:61]\n",
    "macro_features = features_list[61:69]\n",
    "\n",
    "interaction_table = pd.DataFrame(data = None, index = table_for_interaction.index)\n",
    "\n",
    "from IPython.utils import io\n",
    "with io.capture_output() as captured: # to ignore warnings\n",
    "    for i in single_features:\n",
    "        for j in macro_features:\n",
    "            interaction_table[i + '+' + j] = table_for_interaction[i] * table_for_interaction[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_month_permno_num = pd.DataFrame(Counter(total_df.reset_index().date).items(), columns=['date', 'number_of_permno'])\n",
    "# each_month_permno_num.plot(x='date', y='number_of_permno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_total_df = pd.concat([total_df, interaction_table], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_list = new_total_df.columns\n",
    "remove_list = {'gvkey', 'permno', 'ret','sic','exchcd','shrcd','b/m', 'tbl', 'ntis', 'svar', 'd/p', 'e/p', 'dfy', 'tms'}\n",
    "new_features_list = [ele for ele in new_features_list if ele not in remove_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total = new_total_df[new_features_list]\n",
    "y_total = new_total_df['ret']\n",
    "permno_total = new_total_df['permno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_preprocess(X_total, y_total, permno_total, training_len, val_len, testing_len):\n",
    "    \"\"\"split our data into training set, validation set, and testing set\n",
    "\n",
    "    Args:\n",
    "        X_total (dataframe)\n",
    "        y_total (dataframe)\n",
    "        training_len (int): length of years of training set\n",
    "        val_len (int)\n",
    "        testing_len (int)\n",
    "    return: dicts:X_train, y_train, X_val, y_val, X_test, y_test\n",
    "            and a permno series for testing set, which would be part of the result\n",
    "    \"\"\"\n",
    "    X_train_dict = {}\n",
    "    y_train_dict = {}\n",
    "    X_val_dict = {}\n",
    "    y_val_dict = {}\n",
    "    X_test_dict = {}\n",
    "    y_test_dict = {}\n",
    "    permno_test_dict = {}\n",
    "    first_train_end_year = y_total.index.year[0] + training_len - 1\n",
    "    total_len = y_total.index.year[-1] - y_total.index.year[0] + 1\n",
    "    last_train_end_year = first_train_end_year + total_len -(training_len+val_len+testing_len)\n",
    "    train_end_years = np.arange(first_train_end_year, last_train_end_year+1)\n",
    "    print('each end year of training set is:', train_end_years)\n",
    "    for y in tqdm(train_end_years, desc='spliting data', ascii='>>'):\n",
    "        train_start_year = y - training_len + 1\n",
    "        train_end_year = y\n",
    "        val_start_year = y + 1\n",
    "        val_end_year = val_start_year+val_len-1\n",
    "        test_start_year= val_end_year+1\n",
    "        test_end_year = test_start_year+testing_len-1\n",
    "\n",
    "        print('train start year is', train_start_year)\n",
    "        print('train end year is', train_end_year)\n",
    "\n",
    "        print('val start year is', val_start_year)\n",
    "        print('val end year is', val_end_year)\n",
    "\n",
    "        print('test start year is', test_start_year)\n",
    "        print('test end year is', test_end_year)\n",
    "\n",
    "        tmp_X_train = X_total[(X_total.index.year>=train_start_year) & (X_total.index.year<=train_end_year)]\n",
    "        tmp_y_train = y_total[(y_total.index.year>=train_start_year) & (y_total.index.year<=train_end_year)]\n",
    "        tmp_X_val = X_total[(X_total.index.year>=val_start_year) & (X_total.index.year<=val_end_year)]\n",
    "        tmp_y_val = y_total[(y_total.index.year>=val_start_year) & (y_total.index.year<=val_end_year)]\n",
    "        tmp_X_test = X_total[(X_total.index.year>=test_start_year) & (X_total.index.year<=test_end_year)]\n",
    "        tmp_y_test = y_total[(y_total.index.year>=test_start_year) & (y_total.index.year<=test_end_year)]\n",
    "        tmp_permno_test = permno_total[(permno_total.index.year>=test_start_year) & (permno_total.index.year<=test_end_year)]\n",
    "\n",
    "        X_train_dict[y] = tmp_X_train\n",
    "        y_train_dict[y] = tmp_y_train\n",
    "        X_val_dict[y] = tmp_X_val\n",
    "        y_val_dict[y] = tmp_y_val\n",
    "        X_test_dict[y] = tmp_X_test\n",
    "        y_test_dict[y] = tmp_y_test\n",
    "        permno_test_dict[y] = tmp_permno_test\n",
    "    return train_end_years, X_train_dict, y_train_dict, X_val_dict, y_val_dict, X_test_dict, y_test_dict, permno_test_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each end year of training set is: [1985 1986 1987 1988]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting data:   0%|>>>>>>>>>>| 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start year is 1972\n",
      "train end year is 1985\n",
      "val start year is 1986\n",
      "val end year is 1991\n",
      "test start year is 1992\n",
      "test end year is 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting data:  25%|>>>>>>>>>>| 1/4 [00:00<00:02,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start year is 1973\n",
      "train end year is 1986\n",
      "val start year is 1987\n",
      "val end year is 1992\n",
      "test start year is 1993\n",
      "test end year is 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting data:  50%|>>>>>>>>>>| 2/4 [00:01<00:01,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start year is 1974\n",
      "train end year is 1987\n",
      "val start year is 1988\n",
      "val end year is 1993\n",
      "test start year is 1994\n",
      "test end year is 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting data:  75%|>>>>>>>>>>| 3/4 [00:02<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start year is 1975\n",
      "train end year is 1988\n",
      "val start year is 1989\n",
      "val end year is 1994\n",
      "test start year is 1995\n",
      "test end year is 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "spliting data: 100%|>>>>>>>>>>| 4/4 [00:03<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_end_years, X_train_dict, y_train_dict, X_val_dict, y_val_dict, X_test_dict, y_test_dict, permno_test_dict = train_val_test_preprocess(X_total, y_total, permno_total, 14, 6, 26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a952188e4bab49300a5758bda39ddc90e91f41f35dfe6ea820e496e515be371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
